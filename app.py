import streamlit as st
import os
import json
import time
import google.generativeai as genai
from elevenlabs.client import ElevenLabs
from moviepy.editor import VideoFileClip, AudioFileClip

# --- 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ù‡ÙˆÙŠØ© ---
st.set_page_config(
    page_title="Ø§Ø³ØªÙˆØ¯ÙŠÙˆ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø°ÙƒÙŠ",
    page_icon="ğŸ¬",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# Ø¯Ø§Ù„Ø© Ù„Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ± (Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¨ØµØ±ÙŠØ©)
def render_header(image_name, alt_text):
    if os.path.exists(image_name):
        st.image(image_name, use_column_width=True)
    else:
        st.header(alt_text)

# --- 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ---
def load_api_keys():
    try:
        GOOGLE_KEY = st.secrets["GOOGLE_API_KEY"]
        ELEVEN_KEY = st.secrets["ELEVENLABS_API_KEY"]
        return GOOGLE_KEY, ELEVEN_KEY
    except:
        st.error("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¶Ø¹ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙÙŠ Ù…Ù„Ù secrets.toml")
        st.stop()

GOOGLE_API_KEY, ELEVENLABS_API_KEY = load_api_keys()

genai.configure(api_key=GOOGLE_API_KEY)
eleven_client = ElevenLabs(api_key=ELEVENLABS_API_KEY)

# --- 3. Ø¯Ø§Ù„Ø© Ø§Ø®ØªÙŠØ§Ø± Ù…ÙˆØ¯ÙŠÙ„ Gemini Ø§Ù„Ù…ØªØ§Ø­ (Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© NotFound) ---
@st.cache_resource
def get_working_model_name():
    candidates = [
        "gemini-1.5-flash", 
        "models/gemini-1.5-flash", 
        "gemini-1.5-pro", 
        "models/gemini-1.5-pro", 
        "gemini-pro"
    ]
    try:
        available_models = [m.name for m in genai.list_models()]
        for c in candidates:
            if c in available_models or f"models/{c}" in available_models:
                return c
    except:
        pass
    return "gemini-1.5-flash"

CURRENT_MODEL_NAME = get_working_model_name()

# --- 4. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Session State) ---
if 'analysis_done' not in st.session_state: st.session_state['analysis_done'] = False
if 'clips_data' not in st.session_state: st.session_state['clips_data'] = []
if 'dubbed_video' not in st.session_state: st.session_state['dubbed_video'] = None
if 'generated_clips' not in st.session_state: st.session_state['generated_clips'] = []

# --- 5. Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø³Ø±Ø¹Ø©) ---

def check_video_duration(video_path, max_minutes=5):
    try:
        clip = VideoFileClip(video_path)
        dur = clip.duration
        clip.close()
        if dur > max_minutes * 60: return False, dur
        return True, dur
    except: return True, 0

def extract_audio(video_path):
    video = VideoFileClip(video_path)
    audio_path = "temp_audio.mp3"
    video.audio.write_audiofile(audio_path, logger=None)
    video.close()
    return audio_path

def detect_speaker_gender(audio_path):
    model = genai.GenerativeModel(CURRENT_MODEL_NAME)
    try:
        with open(audio_path, "rb") as f: audio_data = f.read()
        prompt = "Listen to the voice. Identify the gender of the MAIN speaker. Return ONLY 'Male' or 'Female'."
        response = model.generate_content([prompt, {"mime_type": "audio/mp3", "data": audio_data}])
        if "female" in response.text.strip().lower(): return "female"
        return "male"
    except: return "male"

def transcribe_and_translate(audio_path, target_lang):
    model = genai.GenerativeModel(CURRENT_MODEL_NAME)
    try:
        with open(audio_path, "rb") as f: audio_data = f.read()
        prompt = f"Transcribe the speech and translate it to {target_lang}. Return ONLY the translated text."
        response = model.generate_content([prompt, {"mime_type": "audio/mp3", "data": audio_data}])
        return response.text
    except: return None

def generate_dubbed_audio(text, voice_id):
    try:
        audio_generator = eleven_client.text_to_speech.convert(
            text=text, 
            voice_id=voice_id, 
            model_id="eleven_multilingual_v2"
        )
        save_path = "dubbed_audio.mp3"
        with open(save_path, "wb") as f:
            for chunk in audio_generator: f.write(chunk)
        return save_path
    except: return None

def merge_audio_video(video_path, audio_path):
    video = VideoFileClip(video_path)
    new_audio = AudioFileClip(audio_path)
    final_video = video.set_audio(new_audio)
    
    # Ù‚Øµ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØµÙˆØª Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø£Ù‚ØµØ±
    if new_audio.duration < video.duration:
        final_video = final_video.subclip(0, new_audio.duration)
    
    output_path = "final_dubbed_video.mp4"
    
    # --- ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© Ù‡Ù†Ø§ (Ultrafast) ---
    final_video.write_videofile(
        output_path, 
        codec="libx264", 
        audio_codec="aac", 
        preset="ultrafast",  # Ø§Ù„Ø³Ø± ÙÙŠ Ø§Ù„Ø³Ø±Ø¹Ø©
        threads=4,          # Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„ Ø§Ù„Ø£Ù†ÙˆÙŠØ©
        logger=None
    )
    
    video.close()
    new_audio.close()
    return output_path

def analyze_video_for_clips(video_path):
    # Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙ‚Ø·
    model = genai.GenerativeModel(CURRENT_MODEL_NAME)
    try:
        myfile = genai.upload_file(video_path)
        while myfile.state.name == "PROCESSING":
            time.sleep(1)
            myfile = genai.get_file(myfile.name)
            
        prompt = """
        Analyze this video. Identify the MOST viral and engaging segments (Shorts/Reels).
        - Duration: 15 to 60 seconds.
        - Return a valid JSON list: [{"start": 10, "end": 40, "label": "Topic Name"}, ...]
        - If no good clips found, return empty list.
        """
        response = model.generate_content([prompt, myfile])
        text = response.text.replace("```json", "").replace("```", "").strip()
        return json.loads(text)
    except: return []

def cut_clips_processing(original_video_path, timestamps):
    # Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªÙ†ÙÙŠØ° (Ø§Ù„Ù‚Øµ Ø§Ù„Ø³Ø±ÙŠØ¹)
    video = VideoFileClip(original_video_path)
    generated_files = []
    for i, item in enumerate(timestamps):
        try:
            start, end = item.get('start'), item.get('end')
            label = item.get('label', f'Clip {i+1}')
            clip = video.subclip(start, end)
            name = f"clip_{i}_{label}.mp4"
            
            # --- ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© Ù‡Ù†Ø§ Ø£ÙŠØ¶Ø§Ù‹ ---
            clip.write_videofile(
                name, 
                codec="libx264", 
                audio_codec="aac", 
                preset="ultrafast",
                threads=4,
                logger=None
            )
            generated_files.append({"path": name, "label": label})
        except: continue
    video.close()
    return generated_files

# --- 6. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (UI) ---

# Ø§Ù„Ø¨Ø§Ù†Ø±
render_header("banner.jpg", "Ø§Ø³ØªÙˆØ¯ÙŠÙˆ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø°ÙƒÙŠ")
st.caption("Ø£ØªÙ…ØªØ© ØµÙ†Ø§Ø¹Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠ")

# Ù‚Ø³Ù… Ø§Ù„Ø±ÙØ¹
st.markdown("### 1. Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")
upload_option = st.radio("Ø§Ù„Ù…ØµØ¯Ø±:", ["Ø±ÙØ¹ Ù…Ù„Ù", "ÙÙŠØ¯ÙŠÙˆ ØªØ¬Ø±ÙŠØ¨ÙŠ (Demo)"], horizontal=True)
video_path = None

if upload_option == "Ø±ÙØ¹ Ù…Ù„Ù":
    uploaded_file = st.file_uploader("Ù…Ù„Ù MP4", type=["mp4"])
    if uploaded_file:
        with open("temp_video.mp4", "wb") as f: f.write(uploaded_file.getbuffer())
        video_path = "temp_video.mp4"
elif upload_option == "ÙÙŠØ¯ÙŠÙˆ ØªØ¬Ø±ÙŠØ¨ÙŠ (Demo)":
    if st.button("ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠ") and os.path.exists("sample.mp4"):
        video_path = "sample.mp4"
        st.success("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠ!")

if video_path:
    # ÙØ­Øµ Ø§Ù„Ù…Ø¯Ø©
    valid, dur = check_video_duration(video_path, 5)
    if not valid:
        st.error(f"âš ï¸ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ø§Ù‹ ({int(dur/60)} Ø¯Ù‚ÙŠÙ‚Ø©). ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠØ¯ÙŠÙˆ Ø£Ù‚ØµØ± Ù„Ù„Ø¹Ø±Ø¶.")
    else:
        st.video(video_path)
        st.divider()
        
        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ ---
        c1, c2 = st.columns(2)
        
        with c1:
            render_header("dubbing.png", "ğŸ™ï¸ Ø§Ù„Ø¯Ø¨Ù„Ø¬Ø©")
            enable_dubbing = st.checkbox("ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø¯Ø¨Ù„Ø¬Ø©")
            
            # Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© (29 Ù„ØºØ©)
            all_languages = [
                "Arabic", "English", "French", "Spanish", "German", 
                "Chinese", "Japanese", "Hindi", "Italian", "Portuguese", 
                "Russian", "Turkish", "Korean", "Dutch", "Swedish", 
                "Indonesian", "Vietnamese", "Filipino", "Ukrainian", 
                "Greek", "Czech", "Finnish", "Romanian", "Danish", 
                "Bulgarian", "Malay", "Slovak", "Croatian", "Polish"
            ]
            target_lang = st.selectbox("Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©", all_languages)
            
        with c2:
            render_header("clipping.png", "âœ‚ï¸ Ø§Ù„Ù‚Øµ Ø§Ù„Ø°ÙƒÙŠ")
            enable_clipping = st.checkbox("Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹")

        # Ø²Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„
        if st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©)", use_container_width=True):
            st.session_state['analysis_done'] = False
            st.session_state['clips_data'] = []
            
            with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨ÙˆØ§Ø³Ø·Ø© Gemini AI..."):
                if enable_clipping:
                    clips_found = analyze_video_for_clips(video_path)
                    st.session_state['clips_data'] = clips_found
                st.session_state['analysis_done'] = True

        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ù„ØªÙ†ÙÙŠØ° ---
        if st.session_state['analysis_done']:
            st.divider()
            st.info("ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
            
            if enable_dubbing:
                st.write("âœ… Ø®Ø¯Ù…Ø© Ø§Ù„Ø¯Ø¨Ù„Ø¬Ø©: Ø¬Ø§Ù‡Ø²Ø©.")
            
            if enable_clipping:
                count = len(st.session_state['clips_data'])
                if count > 0:
                    st.success(f"ÙˆØ¬Ø¯Ù†Ø§ {count} Ù…Ù‚Ø§Ø·Ø¹ Ù…Ø±Ø´Ø­Ø© Ù„Ù„Ø§Ù†ØªØ´Ø§Ø±.")
                    st.json(st.session_state['clips_data'])
                else:
                    st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ Ù‚ÙˆÙŠØ©ØŒ Ù„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©.")
            
            st.divider()
            
            # Ø²Ø± Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            if st.button("ğŸš€ ØªÙ†ÙÙŠØ° Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª (Start Processing)", type="primary", use_container_width=True):
                st.session_state['dubbed_video'] = None
                st.session_state['generated_clips'] = []
                
                with st.status("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ Ø§Ù„Ø§Ø³ØªÙˆØ¯ÙŠÙˆ...", expanded=True) as status:
                    
                    # 1. ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ø¨Ù„Ø¬Ø©
                    if enable_dubbing:
                        status.write("ğŸ™ï¸ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‡ÙˆÙŠØ©...")
                        aud = extract_audio(video_path)
                        gend = detect_speaker_gender(aud)
                        
                        status.write(f"ğŸ“ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ±Ø¬Ù…Ø© ({target_lang})...")
                        txt = transcribe_and_translate(aud, target_lang)
                        
                        if txt:
                            status.write("ğŸ—£ï¸ Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª Ø§Ù„Ø¬Ø¯ÙŠØ¯ (ElevenLabs)...")
                            voice = "21m00Tcm4TlvDq8ikWAM" if gend == "female" else "pNInz6obpgDQGcFmaJgB"
                            dub = generate_dubbed_audio(txt, voice)
                            
                            if dub:
                                status.write("ğŸ¬ Ø¬Ø§Ø±ÙŠ Ø¯Ù…Ø¬ Ø§Ù„ØµÙˆØª Ù…Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù…)...")
                                final_dub = merge_audio_video(video_path, dub)
                                st.session_state['dubbed_video'] = final_dub
                    
                    # 2. ØªÙ†ÙÙŠØ° Ø§Ù„Ù‚Øµ
                    if enable_clipping and st.session_state['clips_data']:
                        status.write("âœ‚ï¸ Ø¬Ø§Ø±ÙŠ Ù‚Øµ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ ÙˆØªØµØ¯ÙŠØ±Ù‡Ø§...")
                        clips = cut_clips_processing(video_path, st.session_state['clips_data'])
                        st.session_state['generated_clips'] = clips
                    
                    status.update(label="âœ… ØªÙ…Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!", state="complete")

# --- Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ---
if st.session_state['dubbed_video']:
    st.header("ğŸ¥ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…Ø¯Ø¨Ù„Ø¬")
    st.video(st.session_state['dubbed_video'])
    with open(st.session_state['dubbed_video'], "rb") as f:
         st.download_button("ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…Ø¯Ø¨Ù„Ø¬", f, file_name="dubbed_video.mp4")

if st.session_state['generated_clips']:
    st.header("ğŸ”¥ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©")
    for clip in st.session_state['generated_clips']:
        st.write(f"**ğŸ“Œ {clip['label']}**")
        st.video(clip['path'])
        with open(clip['path'], "rb") as f:
            st.download_button(f"ØªØ­Ù…ÙŠÙ„ {clip['label']}", f, file_name=clip['path'])